{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Paths (NE RIEN CHANGER)\n",
    "input_dir = '/kaggle/input/le-titanic-spatial'\n",
    "output_dir = '/kaggle/working'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition de fonctions\n",
    "\n",
    "On définit, ici, pleins de fonctions qui pourraient nous être utile pour l'entraînement des modèles. \n",
    "\n",
    "Cela permet de simplifier le processur d'entraînement. Normalement, on placerait plutôt ces fonctions dans un fichier python. Mais pour simplifier, elles sont toutes disponibles dans ce notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_dir: str):\n",
    "    \"\"\"\n",
    "    Charge les données de fichiers CSV de formation et de test.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        Le répertoire contenant les fichiers 'train.csv' et 'test.csv'.\n",
    "\n",
    "    Retourne\n",
    "    -------\n",
    "    tuple\n",
    "        Un tuple contenant deux DataFrames pandas : (train, test).\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(os.path.join(input_dir, 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(input_dir, 'test.csv'))\n",
    "    return train, test\n",
    "\n",
    "def prepare_data(train: pd.DataFrame, test:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prépare les données d'entraînement et de test en ajoutant des transformations et des informations supplémentaires.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    train : pd.DataFrame\n",
    "        Le DataFrame contenant les données d'entraînement.\n",
    "    test : pd.DataFrame\n",
    "        Le DataFrame contenant les données de test.\n",
    "\n",
    "    Retours\n",
    "    -------\n",
    "    tuple\n",
    "        Un tuple contenant les DataFrames d'entraînement et de test modifiés.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Ajoute les colonnes log-transformées pour 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', et 'VRDeck'.\n",
    "    - Ajoute des informations sur les cabines en séparant la colonne 'Cabin' en 'Deck', 'Num', et 'Side'.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Add log to some columns\n",
    "    num_cols_log = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "    # Si on rajoute de nouvelles features dans le train set, on les rajoute directement dans le test set\n",
    "    for col in num_cols_log:\n",
    "        train[col + '_log'] = np.log(train[col] + 1)\n",
    "        test[col + '_log'] = np.log(test[col] + 1)\n",
    "\n",
    "    # Add Cabin information\n",
    "    splitted_values_train = train['Cabin'].apply(split_cabin)\n",
    "    splitted_values_test = test['Cabin'].apply(split_cabin)\n",
    "\n",
    "    # Deck\n",
    "    train['Deck'] = splitted_values_train.apply(lambda x: x[0])\n",
    "    test['Deck'] = splitted_values_test.apply(lambda x: x[0])\n",
    "\n",
    "    # Num\n",
    "    train['Num'] = splitted_values_train.apply(lambda x: x[1])\n",
    "    test['Num'] = splitted_values_test.apply(lambda x: x[1])\n",
    "\n",
    "    # Side\n",
    "    train['Side'] = splitted_values_train.apply(lambda x: x[2])\n",
    "    test['Side'] = splitted_values_test.apply(lambda x: x[2])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def split_cabin(cabin_encoded: str):\n",
    "    \"\"\"\n",
    "    Divise une chaîne de caractères représentant une cabine en trois parties distinctes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cabin_encoded : str\n",
    "        La chaîne de caractères encodée représentant la cabine.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Une liste contenant trois éléments : \n",
    "        - Le premier élément est une chaîne de caractères représentant la première partie de la cabine.\n",
    "        - Le deuxième élément est un entier représentant la deuxième partie de la cabine.\n",
    "        - Le troisième élément est une chaîne de caractères représentant la troisième partie de la cabine.\n",
    "        Si `cabin_encoded` est NaN, retourne [np.nan, np.nan, np.nan].\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(cabin_encoded):\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    \n",
    "    else:\n",
    "\n",
    "        splitted_values = cabin_encoded.split('/')\n",
    "\n",
    "        # La valeur du milieu est un chiffre, on va donc le \"caster\" directement\n",
    "\n",
    "        return splitted_values[0], int(splitted_values[1]), splitted_values[2]\n",
    "    \n",
    "def apply_preprocessing(train: pd.DataFrame, test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Applique le prétraitement aux ensembles de données d'entraînement et de test.\n",
    "    Cette fonction prend en entrée deux DataFrames, `train` et `test`, et applique des transformations\n",
    "    numériques et catégorielles aux caractéristiques spécifiées. Elle renvoie les ensembles de données\n",
    "    prétraités ainsi que la cible d'entraînement.\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pd.DataFrame\n",
    "        Le DataFrame contenant les données d'entraînement.\n",
    "    test : pd.DataFrame\n",
    "        Le DataFrame contenant les données de test.\n",
    "    Returns\n",
    "    -------\n",
    "    features_train_preprocessed : np.ndarray\n",
    "        Les caractéristiques d'entraînement après prétraitement.\n",
    "    features_test_preprocessed : np.ndarray\n",
    "        Les caractéristiques de test après prétraitement.\n",
    "    target_train : pd.Series\n",
    "        La cible d'entraînement.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_features = ['Age', 'RoomService_log', 'FoodCourt_log', 'ShoppingMall_log', 'Spa_log', 'VRDeck_log', 'Num']\n",
    "    cat_features = ['HomePlanet', 'Deck', 'Side', 'Destination']\n",
    "    # Les valeurs booléennes sont que des 0 et des 1 => pas besoin de faire de transformation\n",
    "    bool_features = ['VIP', 'CryoSleep']\n",
    "    target = 'Transported'\n",
    "\n",
    "    features_train = train[num_features + cat_features + bool_features]\n",
    "    features_test = test[num_features + cat_features + bool_features]\n",
    "    target_train = train[target]\n",
    "\n",
    "    # Numerical transformer\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Categorical transformer\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Le ColumnTransformer est une pipeline qui me permet d'appliquer les transformations sur tous les datasets que je peux avoir à utiliser\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, num_features),\n",
    "            ('cat', categorical_transformer, cat_features)\n",
    "        ])\n",
    "    \n",
    "    features_train_preprocessed = preprocessor.fit_transform(features_train)\n",
    "    features_test_preprocessed = preprocessor.transform(features_test)\n",
    "\n",
    "    return features_train_preprocessed, features_test_preprocessed, target_train\n",
    "\n",
    "def prepare_kaggle_submission(file_name, test, predictions):\n",
    "    \"\"\"\n",
    "    Prépare un fichier de soumission pour Kaggle.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Le nom du fichier de soumission à créer.\n",
    "    test : pandas.DataFrame\n",
    "        Le DataFrame contenant les données de test, incluant la colonne 'PassengerId'.\n",
    "    predictions : array-like\n",
    "        Les prédictions du modèle à soumettre, correspondant aux données de test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Cette fonction ne retourne rien. Elle crée un fichier CSV de soumission.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(f\"Création du fichier de soumission {file_name}\")\n",
    "    submission = pd.DataFrame({'PassengerId': test.PassengerId, 'Transported': predictions})\n",
    "\n",
    "    submission['Transported'] = submission['Transported'].astype(bool)\n",
    "\n",
    "    submission.to_csv(os.path.join(output_dir, file_name), index=False)\n",
    "\n",
    "def train_model_all_data_and_predict(model, features_train, target_train, features_test):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle sur toutes les données d'entraînement et effectue des prédictions sur les données de test.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Le modèle à entraîner. Doit implémenter les méthodes `fit` et `predict`.\n",
    "    features_train : array-like\n",
    "        Les caractéristiques des données d'entraînement.\n",
    "    target_train : array-like\n",
    "        Les cibles des données d'entraînement.\n",
    "    features_test : array-like\n",
    "        Les caractéristiques des données de test.\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like\n",
    "        Les prédictions du modèle sur les données de test.\n",
    "    \"\"\"\n",
    "\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions = model.predict(features_test)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def do_cross_validation(model, features_train, target_train, cv=5):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée sur le modèle donné.\n",
    "    Paramètres\n",
    "    ----------\n",
    "    model : estimator\n",
    "        Le modèle à valider.\n",
    "    features_train : array-like\n",
    "        Les caractéristiques d'entraînement.\n",
    "    target_train : array-like\n",
    "        Les cibles d'entraînement.\n",
    "    cv : int, optionnel\n",
    "        Le nombre de plis pour la validation croisée (par défaut 5).\n",
    "    Affiche\n",
    "    -------\n",
    "    Cross-validation scores : array\n",
    "        Les scores de validation croisée pour chaque pli.\n",
    "    Mean CV accuracy : float\n",
    "        La précision moyenne de la validation croisée.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_val_score(model, features_train, target_train, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores: {scores}\")\n",
    "    print(f\"Mean CV accuracy: {scores.mean():.2%}\")\n",
    "\n",
    "def test_for_overfitting(model, features_train, target_train, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Teste le surapprentissage d'un modèle en comparant les performances sur les ensembles\n",
    "    d'entraînement et de test.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        Le modèle à entraîner et à tester. Doit implémenter les méthodes `fit` et `predict`.\n",
    "    features_train : array-like\n",
    "        Les caractéristiques d'entraînement.\n",
    "    target_train : array-like\n",
    "        Les cibles d'entraînement.\n",
    "    test_size : float, optional\n",
    "        La proportion de l'ensemble de données à inclure dans l'ensemble de test (default is 0.2).\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Cette fonction ne retourne rien. Elle affiche les rapports de classification pour les ensembles\n",
    "        d'entraînement et de test.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_train, target_train, test_size=test_size)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    print(\"Résultats sur le train set\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    print()\n",
    "    print(\"Résultats sur le test set\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "def tune_hyperparameters(model, features_train, target_train, param_distributions, cv=5, n_iter=20):\n",
    "    \"\"\"\n",
    "    Tune les hyperparamètres d'un modèle en utilisant une recherche aléatoire.\n",
    "    Cette fonction utilise RandomizedSearchCV pour effectuer une recherche aléatoire\n",
    "    sur les hyperparamètres spécifiés et trouver la meilleure combinaison pour le modèle donné.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : estimator object\n",
    "        Le modèle pour lequel les hyperparamètres doivent être optimisés.\n",
    "    features_train : array-like or DataFrame\n",
    "        Les caractéristiques d'entraînement.\n",
    "    target_train : array-like or Series\n",
    "        La cible d'entraînement.\n",
    "    param_distributions : dict\n",
    "        Le dictionnaire contenant les distributions des hyperparamètres à tester.\n",
    "    cv : int, default=5\n",
    "        Le nombre de folds pour la validation croisée.\n",
    "    n_iter : int, default=100\n",
    "        Le nombre d'itérations pour la recherche aléatoire.\n",
    "    Returns\n",
    "    -------\n",
    "    best_estimator_ : estimator object\n",
    "        Le modèle avec les meilleurs hyperparamètres trouvés.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.ensemble import RandomForestClassifier\n",
    "    >>> param_distributions = {\n",
    "    ...     'n_estimators': [100, 200, 300],\n",
    "    ...     'max_depth': [None, 10, 20, 30]\n",
    "    ... }\n",
    "    >>> model = RandomForestClassifier()\n",
    "    >>> best_model = tune_hyperparameters(model, X_train, y_train, param_distributions)\n",
    "    Best params: {'max_depth': 20, 'n_estimators': 200}\n",
    "    Best score: 0.85\n",
    "    \"\"\"\n",
    "\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_distributions, n_iter=n_iter, cv=cv, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "    random_search.fit(features_train, target_train)\n",
    "\n",
    "    print(f\"Best params: {random_search.best_params_}\")\n",
    "    print(f\"Best score: {random_search.best_score_}\")\n",
    "\n",
    "    return random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "On utilise les fonctions disponibles pour préparer les données afin de pouvoir entraîner les modèles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data(input_dir)\n",
    "\n",
    "train, test = prepare_data(train, test)\n",
    "\n",
    "features_train_preprocessed, features_test_preprocessed, target_train = apply_preprocessing(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement d'un RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.7533065  0.7573318  0.79413456 0.81875719 0.78711162]\n",
      "Mean CV accuracy: 78.21%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# On fait une CV pour savoir si notre modèle prédit correctement\n",
    "do_cross_validation(model, features_train_preprocessed, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats sur le train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      3443\n",
      "        True       1.00      1.00      1.00      3511\n",
      "\n",
      "    accuracy                           1.00      6954\n",
      "   macro avg       1.00      1.00      1.00      6954\n",
      "weighted avg       1.00      1.00      1.00      6954\n",
      "\n",
      "\n",
      "Résultats sur le test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.83      0.81       872\n",
      "        True       0.82      0.77      0.80       867\n",
      "\n",
      "    accuracy                           0.80      1739\n",
      "   macro avg       0.80      0.80      0.80      1739\n",
      "weighted avg       0.80      0.80      0.80      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On peut tester si notre modèle overfit\n",
    "test_for_overfitting(model, features_train_preprocessed, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   3.7s[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.8s\n",
      "\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   2.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   3.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   8.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   6.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   8.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   9.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=   9.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  10.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  10.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  15.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  12.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  10.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  10.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  10.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   9.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   9.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  21.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=   9.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   9.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  19.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  21.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  18.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  11.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  12.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  12.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  11.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  21.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   7.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  12.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  24.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   6.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   6.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   3.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   6.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   5.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  14.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   7.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   6.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  17.9s\n",
      "Best params: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 10}\n",
      "Best score: 0.7963921833838343\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Avec un RandomSearch (pour éviter de chercher tous les paramètres), on peut trouver les meilleurs paramètres\n",
    "best_model = tune_hyperparameters(model, features_train_preprocessed, target_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats sur le train set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.84      0.85      3454\n",
      "        True       0.85      0.87      0.86      3500\n",
      "\n",
      "    accuracy                           0.86      6954\n",
      "   macro avg       0.86      0.86      0.86      6954\n",
      "weighted avg       0.86      0.86      0.86      6954\n",
      "\n",
      "\n",
      "Résultats sur le test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.81      0.80       861\n",
      "        True       0.81      0.80      0.80       878\n",
      "\n",
      "    accuracy                           0.80      1739\n",
      "   macro avg       0.80      0.80      0.80      1739\n",
      "weighted avg       0.80      0.80      0.80      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On test le meilleur modèle pour voir s'il overfit\n",
    "test_for_overfitting(best_model, features_train_preprocessed, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création du fichier de soumission best_RF.csv\n"
     ]
    }
   ],
   "source": [
    "# Ré-entraînement du modèle et prédiction sur le test set\n",
    "predictions = train_model_all_data_and_predict(best_model, features_train_preprocessed, target_train, features_test_preprocessed)\n",
    "\n",
    "# Création du fichier de soumission\n",
    "prepare_kaggle_submission('best_RF.csv', test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-bahut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
